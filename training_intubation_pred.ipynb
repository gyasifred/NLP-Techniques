{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1gRv6afVW0epyx8qLAXrWBczss2xbMmDW",
      "authorship_tag": "ABX9TyMCqQfRDOU13vBuIeChxs/O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/NLP-Techniques/blob/main/training_intubation_pred.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "pIrGs5k7fE6s",
        "outputId": "b2b17773-ebe1-4cfb-f1f1-cf7328273134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a97fe26de778>\u001b[0m in \u001b[0;36m<cell line: 319>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-a97fe26de778>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# Load and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading training data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading validation data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-a97fe26de778>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(data_dir, split)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mtime_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mtime_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_series\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;31m# Ensure all inputs are in the correct dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mrepeat\u001b[0;34m(a, repeats, axis)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \"\"\"\n\u001b[0;32m--> 466\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'repeat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Concatenate, MultiHeadAttention, LayerNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from transformers import TFBertModel\n",
        "from datetime import datetime\n",
        "from tensorflow.keras import mixed_precision\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "# Configuration Parameters (Hardcoded)\n",
        "data_dir = '/content/drive/MyDrive/CLINLPPROJ/processed_data'  # Directory containing the processed data files\n",
        "model_dir = \"models/clinical_prediction\"  # Directory for model and logs\n",
        "batch_size = 2  # Batch size for training\n",
        "epochs = 2  # Number of epochs\n",
        "learning_rate = 1e-4  # Learning rate\n",
        "sequence_length = 128  # Sequence length for time-series data\n",
        "time_series_features = 24  # Number of time-series features\n",
        "static_features = 21  # Number of static features\n",
        "bert_model_name = \"emilyalsentzer/Bio_ClinicalBERT\"  # Pretrained BERT model\n",
        "bert_dropout_rate = 0.2  # Dropout rate for BERT output\n",
        "bert_input_size = 512  # BERT input size\n",
        "lstm_sizes = [64, 32, 32]  # LSTM layer sizes\n",
        "lstm_dropout = 0.2  # Dropout rate for LSTM layers\n",
        "mlp_sizes = [128, 64]  # MLP layer sizes\n",
        "mlp_dropout = 0.4  # Dropout rate for MLP layers\n",
        "fusion_dropout = 0.4  # Dropout rate for fusion layers\n",
        "num_attention_heads = 12  # Number of attention heads in fusion layer\n",
        "fusion_key_dim = 64  # Key dimension for multi-head attention\n",
        "gpu = \"1\"  # GPU to use (set -1 for CPU)\n",
        "\n",
        "class BERTLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, bert_model, output_size, dropout_rate, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.bert = bert_model\n",
        "        self.dense = Dense(output_size, activation=\"relu\")\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        return config\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_ids, attention_mask = inputs\n",
        "        input_ids = tf.cast(input_ids, dtype=tf.int32)\n",
        "        attention_mask = tf.cast(attention_mask, dtype=tf.int32)\n",
        "\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
        "        sequence_output = bert_output[0]\n",
        "        pooled_output = sequence_output[:, 0, :]\n",
        "        x = self.dense(pooled_output)\n",
        "        x = self.dropout(x, training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FusionLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_heads, key_dim, dropout_rate, fusion_dim=128, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.fusion_dim = fusion_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Project all inputs to the same dimension\n",
        "        self.lstm_projection = Dense(self.fusion_dim, activation=\"relu\")\n",
        "        self.bert_projection = Dense(self.fusion_dim, activation=\"relu\")\n",
        "        self.mlp_projection = Dense(self.fusion_dim, activation=\"relu\")\n",
        "\n",
        "        self.attention = MultiHeadAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            key_dim=self.key_dim,\n",
        "            dropout=self.dropout_rate\n",
        "        )\n",
        "        self.layer_norm = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'num_heads': self.num_heads,\n",
        "            'key_dim': self.key_dim,\n",
        "            'dropout_rate': self.dropout_rate,\n",
        "            'fusion_dim': self.fusion_dim\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.fusion_dim)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        lstm_output, bert_output, mlp_output = inputs\n",
        "\n",
        "        # Project all inputs to same dimension\n",
        "        lstm_proj = self.lstm_projection(lstm_output)\n",
        "        bert_proj = self.bert_projection(bert_output)\n",
        "        mlp_proj = self.mlp_projection(mlp_output)\n",
        "\n",
        "        # Add sequence dimension\n",
        "        lstm_proj = tf.expand_dims(lstm_proj, axis=1)\n",
        "        bert_proj = tf.expand_dims(bert_proj, axis=1)\n",
        "        mlp_proj = tf.expand_dims(mlp_proj, axis=1)\n",
        "\n",
        "        # Concatenate along sequence dimension\n",
        "        concat_output = tf.concat([lstm_proj, bert_proj, mlp_proj], axis=1)\n",
        "\n",
        "        # Self-attention\n",
        "        attention_output = self.attention(\n",
        "            query=concat_output,\n",
        "            value=concat_output,\n",
        "            key=concat_output,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        # Residual connection and normalization\n",
        "        normalized_output = self.layer_norm(concat_output + attention_output)\n",
        "\n",
        "        # Global pooling across sequence dimension\n",
        "        output = tf.reduce_mean(normalized_output, axis=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def build_model():\n",
        "    # Initialize BERT\n",
        "    bert_model = TFBertModel.from_pretrained(bert_model_name)\n",
        "    bert_layer = BERTLayer(bert_model, 128, bert_dropout_rate)  # Changed output size to 128\n",
        "\n",
        "    # Define inputs\n",
        "    time_series_input = Input(shape=(sequence_length, time_series_features),\n",
        "                            name=\"time_series_input\")\n",
        "    bert_input_ids = Input(shape=(512,), dtype=tf.int32, name=\"bert_input_ids\")\n",
        "    bert_attention_mask = Input(shape=(512,), dtype=tf.int32, name=\"bert_attention_mask\")\n",
        "    static_input = Input(shape=(static_features,), name=\"static_input\")\n",
        "\n",
        "    # LSTM branch\n",
        "    x_lstm = LSTM(lstm_sizes[0], return_sequences=True)(time_series_input)\n",
        "    x_lstm = LSTM(lstm_sizes[1])(x_lstm)\n",
        "    x_lstm = Dropout(lstm_dropout)(x_lstm)\n",
        "    lstm_output = Dense(128, activation=\"relu\")(x_lstm)  # Changed to 128\n",
        "\n",
        "    # BERT branch\n",
        "    bert_output = bert_layer([bert_input_ids, bert_attention_mask])\n",
        "\n",
        "    # Static features branch\n",
        "    x_mlp = Dense(mlp_sizes[0], activation=\"relu\")(static_input)\n",
        "    x_mlp = Dropout(mlp_dropout)(x_mlp)\n",
        "    mlp_output = Dense(128, activation=\"relu\")(x_mlp)  # Changed to 128\n",
        "\n",
        "    # Fusion\n",
        "    fusion_layer = FusionLayer(\n",
        "        num_heads=num_attention_heads,\n",
        "        key_dim=fusion_key_dim,\n",
        "        dropout_rate=fusion_dropout,\n",
        "        fusion_dim=128  # Set consistent fusion dimension\n",
        "    )\n",
        "    fusion_output = fusion_layer([lstm_output, bert_output, mlp_output])\n",
        "\n",
        "    # Final classification layers\n",
        "    x = Dense(64, activation=\"relu\")(fusion_output)\n",
        "    x = Dropout(0.2)(x)\n",
        "    output = Dense(1, activation=\"sigmoid\", name=\"prediction\")(x)\n",
        "\n",
        "    # Create model with dictionary inputs\n",
        "    model = Model(\n",
        "        inputs={\n",
        "            'time_series_input': time_series_input,\n",
        "            'bert_input_ids': bert_input_ids,\n",
        "            'bert_attention_mask': bert_attention_mask,\n",
        "            'static_input': static_input\n",
        "        },\n",
        "        outputs=output\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Enable mixed precision to reduce memory usage\n",
        "policy = mixed_precision.Policy('mixed_float16')\n",
        "mixed_precision.set_global_policy(policy)\n",
        "\n",
        "def load_data(data_dir, split):\n",
        "    data = np.load(os.path.join(data_dir, f'{split}_data.npz'))\n",
        "\n",
        "    time_series = data['time_series_features'].astype(np.float32)  # Convert to float32\n",
        "    if len(time_series.shape) == 2:\n",
        "        time_series = np.expand_dims(time_series, axis=1)\n",
        "        time_series = np.repeat(time_series, 128, axis=1)\n",
        "\n",
        "    # Ensure all inputs are in the correct dtype\n",
        "    return {\n",
        "        'time_series_features': time_series,\n",
        "        'bert_input_ids': data['bert_input_ids'].astype(np.int32),\n",
        "        'bert_attention_mask': data['bert_attention_mask'].astype(np.int32),\n",
        "        'static_features': data['static_features'].astype(np.float32),\n",
        "        'labels': data['labels'].astype(np.float32)\n",
        "    }\n",
        "\n",
        "def create_dataset(data, batch_size, shuffle=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "        'time_series_input': data['time_series_features'],\n",
        "        'bert_input_ids': data['bert_input_ids'],\n",
        "        'bert_attention_mask': data['bert_attention_mask'],\n",
        "        'static_input': data['static_features'],\n",
        "    }, data['labels']))\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "\n",
        "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load and prepare data\n",
        "    print(\"Loading training data...\")\n",
        "    train_data = load_data(data_dir, 'train')\n",
        "    print(\"Loading validation data...\")\n",
        "    val_data = load_data(data_dir, 'val')\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = create_dataset(train_data, batch_size, shuffle=True)\n",
        "    val_dataset = create_dataset(val_data, batch_size, shuffle=False)\n",
        "\n",
        "    # Print shapes for debugging\n",
        "    for batch in train_dataset.take(1):\n",
        "        inputs, labels = batch\n",
        "        print(\"\\nInput shapes:\")\n",
        "        for key, tensor in inputs.items():\n",
        "            print(f\"{key}: {tensor.shape}\")\n",
        "        print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "    # Build and compile model\n",
        "    print(\"\\nBuilding model...\")\n",
        "    model = build_model()\n",
        "    model.summary()\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[\n",
        "            \"accuracy\",\n",
        "            tf.keras.metrics.AUC(name='auc'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Create model directory and callbacks\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=os.path.join(model_dir, \"best_model.keras\"),\n",
        "            monitor=\"val_loss\",\n",
        "            save_best_only=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\",\n",
        "            patience=5,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_loss\",\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            verbose=1\n",
        "        ),\n",
        "        tf.keras.callbacks.TensorBoard(\n",
        "            log_dir=os.path.join(model_dir, \"logs\", timestamp)\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nStarting training...\")\n",
        "    try:\n",
        "        history = model.fit(\n",
        "            train_dataset,\n",
        "            validation_data=val_dataset,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Save final model\n",
        "        final_model_path = os.path.join(model_dir, \"final_model.keras\")\n",
        "        model.save(final_model_path)\n",
        "        print(f\"\\nModel training complete. Final model saved at {final_model_path}\")\n",
        "\n",
        "        # Save model configuration\n",
        "        model_config_path = os.path.join(model_dir, \"model_config.json\")\n",
        "        model_config = {\n",
        "            \"bert_model_name\": bert_model_name,\n",
        "            \"sequence_length\": sequence_length,\n",
        "            \"time_series_features\": time_series_features,\n",
        "            \"static_features\": static_features,\n",
        "            \"lstm_sizes\": lstm_sizes,\n",
        "            \"lstm_dropout\": lstm_dropout,\n",
        "            \"mlp_sizes\": mlp_sizes,\n",
        "            \"mlp_dropout\": mlp_dropout,\n",
        "            \"fusion_dropout\": fusion_dropout,\n",
        "            \"num_attention_heads\": num_attention_heads,\n",
        "            \"fusion_key_dim\": fusion_key_dim,\n",
        "            \"learning_rate\": learning_rate\n",
        "        }\n",
        "\n",
        "        with open(model_config_path, 'w') as f:\n",
        "            json.dump(model_config, f, indent=4)\n",
        "        print(f\"Model configuration saved at {model_config_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nTraining failed with error: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}