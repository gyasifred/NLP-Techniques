{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df94a73",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how to use the sklearn CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678c62f",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f342f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c26ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/kgyasi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kgyasi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/kgyasi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/kgyasi/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the necessary nltk documents that will be required\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34a652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-23 18:16:59--  https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv\n",
      "Resolving lazyprogrammer.me (lazyprogrammer.me)... 104.21.23.210, 172.67.213.166, 2606:4700:3030::ac43:d5a6, ...\n",
      "Connecting to lazyprogrammer.me (lazyprogrammer.me)|104.21.23.210|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5085081 (4.8M) [text/csv]\n",
      "Saving to: ‘bbc_text_cls.csv’\n",
      "\n",
      "bbc_text_cls.csv    100%[===================>]   4.85M   581KB/s    in 11s     \n",
      "\n",
      "2023-07-23 18:17:13 (453 KB/s) - ‘bbc_text_cls.csv’ saved [5085081/5085081]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the BBC news dataset\n",
    "!wget -nc  https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe8b5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the bbc news datase with pandas\n",
    "bbc = pd.read_csv(\"bbc_text_cls.csv\", encoding= \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7583e48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#previw the dataset\n",
    "bbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f0b7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inputs and labels \n",
    "inputs = bbc.text\n",
    "labels = bbc.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32a1495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a histogram to check the distribution of the labels\n",
    "labels.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d745b0",
   "metadata": {},
   "source": [
    "## Train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1a453e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, y_train, y_test = train_test_split(inputs, labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8780cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create word to index mapping using the CountVectorizer class\n",
    "\n",
    "#with default parameters\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "#fit to train set\n",
    "vectorizer.fit(input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2f71159",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.transform(input_train)\n",
    "x_test = vectorizer.transform(input_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2dd33",
   "metadata": {},
   "source": [
    "Remember that the result is a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a8a603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1780x26762 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 358989 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10254ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 2, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 2, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use can convert to numpy array using  the to_array\n",
    "x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44395ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007536029201223603"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the percentage of the sparse matrix which are non-zeros\n",
    "(x_train != 0).sum() / np.prod(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41eab97",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "158d4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e50f669a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36d82de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9943820224719101\n",
      "Test Score: 0.9730337078651685\n"
     ]
    }
   ],
   "source": [
    "# check model performance on training and test sets\n",
    "print(f'Train Score: {model.score(x_train, y_train)}')\n",
    "print(f'Test Score: {model.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc9665c",
   "metadata": {},
   "source": [
    "Lrt repeat the procedure above with different variation using stop words, lematization, stemming, simple white space split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5e79c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9960674157303371\n",
      "Test Score: 0.9730337078651685\n"
     ]
    }
   ],
   "source": [
    "# with stop words \n",
    "\n",
    "vectorizer = CountVectorizer(stop_words= 'english')\n",
    "#fit to train set\n",
    "vectorizer.fit(input_train)\n",
    "x_train = vectorizer.transform(input_train)\n",
    "x_test = vectorizer.transform(input_test)\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "# check model performance on training and test sets\n",
    "print(f'Train Score: {model.score(x_train, y_train)}')\n",
    "print(f'Test Score: {model.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83cb48f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Map Penn Treebank POS tag to WordNet POS tag for lemmatization.\n",
    "    \n",
    "    Parameters:\n",
    "        treebank_tag (str): The Penn Treebank POS tag.\n",
    "        \n",
    "    Returns:\n",
    "        str: The WordNet POS tag.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "class LemmaTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenize and lemmatize a text document using NLTK.\n",
    "    \n",
    "    This class provides a custom tokenizer that tokenizes the input text\n",
    "    document and then lemmatizes the tokens using WordNetLemmatizer from NLTK.\n",
    "    The Part-of-Speech tags are used to guide the lemmatization process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the LemmaTokenizer.\n",
    "        \n",
    "        This constructor initializes the WordNetLemmatizer used for lemmatization.\n",
    "        \"\"\"\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"\n",
    "        Tokenize and lemmatize the input document.\n",
    "        \n",
    "        Parameters:\n",
    "            doc (str): The text document to tokenize and lemmatize.\n",
    "            \n",
    "        Returns:\n",
    "            list: The list of lemmatized tokens.\n",
    "        \"\"\"\n",
    "        # Tokenize the document\n",
    "        tokens = word_tokenize(doc)\n",
    "        \n",
    "        # Perform POS tagging\n",
    "        words_and_tags = nltk.pos_tag(tokens)\n",
    "        \n",
    "        # Lemmatize the tokens based on their POS tags\n",
    "        return [self.wnl.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in words_and_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "588724fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kgyasi/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9932584269662922\n",
      "Test Score: 0.9730337078651685\n"
     ]
    }
   ],
   "source": [
    "# with lematization\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "#fit to train set\n",
    "vectorizer.fit(input_train)\n",
    "x_train = vectorizer.transform(input_train)\n",
    "x_test = vectorizer.transform(input_test)\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "# check model performance on training and test sets\n",
    "print(f'Train Score: {model.score(x_train, y_train)}')\n",
    "print(f'Test Score: {model.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6c1af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenize and perform stemming on a text document using NLTK's PorterStemmer.\n",
    "    \n",
    "    This class provides a custom tokenizer that tokenizes the input text\n",
    "    document and then performs stemming using the PorterStemmer from NLTK.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the StemTokenizer.\n",
    "        \n",
    "        This constructor initializes the PorterStemmer used for stemming.\n",
    "        \"\"\"\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        \"\"\"\n",
    "        Tokenize and perform stemming on the input document.\n",
    "        \n",
    "        Parameters:\n",
    "            doc (str): The text document to tokenize and stem.\n",
    "            \n",
    "        Returns:\n",
    "            list: The list of stemmed tokens.\n",
    "        \"\"\"\n",
    "        # Tokenize the document\n",
    "        tokens = word_tokenize(doc)\n",
    "        \n",
    "        # Perform stemming on the tokens\n",
    "        return [self.porter.stem(t) for t in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de06b97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kgyasi/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9921348314606742\n",
      "Test Score: 0.9730337078651685\n"
     ]
    }
   ],
   "source": [
    "# with steming\n",
    "vectorizer = CountVectorizer(tokenizer=StemTokenizer())\n",
    "#fit to train set\n",
    "vectorizer.fit(input_train)\n",
    "x_train = vectorizer.transform(input_train)\n",
    "x_test = vectorizer.transform(input_test)\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "# check model performance on training and test sets\n",
    "print(f'Train Score: {model.score(x_train, y_train)}')\n",
    "print(f'Test Score: {model.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28d4e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(s):\n",
    "  return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75ede79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kgyasi/anaconda3/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9977528089887641\n",
      "Test Score: 0.9595505617977528\n"
     ]
    }
   ],
   "source": [
    "# with steming\n",
    "vectorizer = CountVectorizer(tokenizer=simple_tokenizer)\n",
    "#fit to train set\n",
    "vectorizer.fit(input_train)\n",
    "x_train = vectorizer.transform(input_train)\n",
    "x_test = vectorizer.transform(input_test)\n",
    "model = MultinomialNB()\n",
    "model.fit(x_train, y_train)\n",
    "# check model performance on training and test sets\n",
    "print(f'Train Score: {model.score(x_train, y_train)}')\n",
    "print(f'Test Score: {model.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd2db0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
