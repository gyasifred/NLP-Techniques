{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8Lm/RmRUt4t4KQiCz7y1t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/NLP-Techniques/blob/main/Instruction_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup: Authenticate with Google & install the open-source [Lamini library](https://pypi.org/project/lamini) to use LLMs easily\n",
        "%%capture\n",
        "\n",
        "from google.colab import auth\n",
        "import requests\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "def authenticate_powerml():\n",
        "  auth.authenticate_user()\n",
        "  gcloud_token = !gcloud auth print-access-token\n",
        "  powerml_token_response = requests.get('https://api.powerml.co/data_studio/auth/verify_gcloud_token?token=' + gcloud_token[0])\n",
        "  return powerml_token_response.json()['token']\n",
        "\n",
        "production_token = authenticate_powerml()\n",
        "!pip install --upgrade --force-reinstall --ignore-installed lamini\n",
        "\n",
        "keys_dir_path = '/root/.powerml'\n",
        "os.makedirs(keys_dir_path, exist_ok=True)\n",
        "\n",
        "keys_file_path = keys_dir_path + '/configure_llama.yaml'\n",
        "with open(keys_file_path, 'w') as f:\n",
        "  yaml.dump(config, f, default_flow_style=False)\n"
      ],
      "metadata": {
        "id": "FV59DwAJJvFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sLjFxZ67bcN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install jsonline\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import jsonlines\n",
        "from llama import BasicModelRunner\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "atE1m1607l61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the [Alpaca dataset](https://github.com/gururise/AlpacaDataCleaned)"
      ],
      "metadata": {
        "id": "-sfGEc8-FP5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_tuned_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\", streaming=True)"
      ],
      "metadata": {
        "id": "HfxOvsCK7yMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = 5\n",
        "print(\"Instruction-tuned dataset:\")\n",
        "top_m = list(itertools.islice(instruction_tuned_dataset, m))\n",
        "for j in top_m:\n",
        "  print(j)"
      ],
      "metadata": {
        "id": "oXTrbim0FxTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_with_input = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "\n",
        "prompt_template_without_input = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\""
      ],
      "metadata": {
        "id": "IvI9qkJAGTW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_data = []\n",
        "for j in top_m:\n",
        "  if not j[\"input\"]:\n",
        "    processed_prompt = prompt_template_without_input.format(instruction=j[\"instruction\"])\n",
        "  else:\n",
        "    processed_prompt = prompt_template_with_input.format(instruction=j[\"instruction\"], input=j[\"input\"])\n",
        "\n",
        "  processed_data.append({\"input\": processed_prompt, \"output\": j[\"output\"]})\n"
      ],
      "metadata": {
        "id": "llXBRMqYG5t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(processed_data[0])"
      ],
      "metadata": {
        "id": "fk8PsKRcHasO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(f'alpaca_processed.jsonl', 'w') as writer:\n",
        "    writer.write_all(processed_data)"
      ],
      "metadata": {
        "id": "CtA57-ffITN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare non-instruction-tuned vs. instruction-tuned models"
      ],
      "metadata": {
        "id": "oIsA4ovQIvIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path_hf = \"lamini/alpaca\"\n",
        "dataset_hf = load_dataset(dataset_path_hf)\n",
        "print(dataset_hf)"
      ],
      "metadata": {
        "id": "TC4hu391IgCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-hf\")\n",
        "non_instruct_output = non_instruct_model(\"Tell me how to train my dog to sit\")\n",
        "print(\"Not instruction-tuned output (Llama 2 Base):\", non_instruct_output)"
      ],
      "metadata": {
        "id": "hcSvYOXfI3VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VvL83uFQM4DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruct_model = BasicModelRunner(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "instruct_output = instruct_model(\"Tell me how to train my dog to sit\")\n",
        "print(\"Instruction-tuned output (Llama 2): \", instruct_output)"
      ],
      "metadata": {
        "id": "lpGHPpWJJFSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chatgpt = BasicModelRunner(\"chat-gpt\")\n",
        "# instruct_output_chatgpt = chatgpt(\"Tell me how to train my dog to sit\")\n",
        "# print(\"Instruction-tuned output (ChatGPT): \", instruct_output_chatgpt)"
      ],
      "metadata": {
        "id": "SSqhKgkjLYU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try smaller models"
      ],
      "metadata": {
        "id": "HGqHNlU1LgG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")"
      ],
      "metadata": {
        "id": "1ASWimhjLe6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
        "  # Tokenize\n",
        "  input_ids = tokenizer.encode(\n",
        "          text,\n",
        "          return_tensors=\"pt\",\n",
        "          truncation=True,\n",
        "          max_length=max_input_tokens\n",
        "  )\n",
        "\n",
        "  # Generate\n",
        "  device = model.device\n",
        "  generated_tokens_with_prompt = model.generate(\n",
        "    input_ids=input_ids.to(device),\n",
        "    max_length=max_output_tokens\n",
        "  )\n",
        "\n",
        "  # Decode\n",
        "  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "  # Strip the prompt\n",
        "  generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
        "\n",
        "  return generated_text_answer"
      ],
      "metadata": {
        "id": "cWwuL99YLm2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset_path = \"lamini/lamini_docs\"\n",
        "finetuning_dataset = load_dataset(finetuning_dataset_path)\n",
        "print(finetuning_dataset)"
      ],
      "metadata": {
        "id": "UB0KkwOrLx46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample = finetuning_dataset[\"test\"][0]\n",
        "print(test_sample)\n",
        "\n",
        "print(inference(test_sample[\"question\"], model, tokenizer))"
      ],
      "metadata": {
        "id": "wwNwqkKkL5Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare to finetuned small model"
      ],
      "metadata": {
        "id": "K1GPLwBvMH7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")"
      ],
      "metadata": {
        "id": "9S1znN-pL_EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inference(test_sample[\"question\"], instruction_model, tokenizer))"
      ],
      "metadata": {
        "id": "HA4UeyVZMWY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pssst! If you were curious how to upload your own dataset to Huggingface\n",
        "# Here is how we did it\n",
        "\n",
        "# !pip install huggingface_hub\n",
        "# !huggingface-cli login\n",
        "\n",
        "# import pandas as pd\n",
        "# import datasets\n",
        "# from datasets import Dataset\n",
        "\n",
        "# finetuning_dataset = Dataset.from_pandas(pd.DataFrame(data=finetuning_dataset))\n",
        "# finetuning_dataset.push_to_hub(dataset_path_hf)"
      ],
      "metadata": {
        "id": "TyI_cxbJMnIw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}